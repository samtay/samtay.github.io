---
layout: post
title: 'Graphical Models: Part 1'
description: Introduction to probabalistic graphical models
tags: [pgm, probability]
mathjax: true
---

This week I will start my second quarter as a PhD student at the University of
Washington and I will be studying Graphical Models with [Sewoong
Oh](https://homes.cs.washington.edu/~sewoong/) over in the CS department, a class
which I've been looking forward to for a number of reasons, chief among
them being that I like the opportunity to employ (seemingly disparate) pure
math techniques to unsuspecting applied problem domains; in this case,
solving probabilistic inference problems via graph theory.

This is the first of a series designed to introduce the concept of probabalistic graphical models, motivate their definitions, demonstrate their mathematical beauty, and (time-permitting) show their implementation in Haskell.

**Disclaimer**: this *Part 1* introductory post is rooted in the excessively optimistic assumption that I will have time to write subsequent posts that expand from this introduction, and the less excessively optimistic assumption that this expansion can be written to a broader audience than fellow graduate students enrolled in UW CSE 515.

**Prerequisites** are unfortunately yet to be determined. Since I've only just started this class it is impossible to know exactly how much of it can be disseminated to a broader audience. At the very least, you will need
1. Mathematical maturity, at least enough to understand the definition of a [graph](https://en.wikipedia.org/wiki/Graph_theory) <!-- or link to my own definition -->
2. Probability at the [undergraduate level](https://projects.iq.harvard.edu/stat110/home).

# Motivation

Let \\(\mathcal{X}\\) be some alphabet[^1] and consider a probability
distribution over the sample space \\(\mathcal{X}^n\\) with probability mass
function \\(\mu\\). That is, letting \\((X_1,\ldots,X_n)\\) denote a random
vector over this distribution,

$$ \mu(x_1, \ldots, x_n) = \mathbb{P}(X_1 = x_1, \ldots, X_n = x_n). $$

In this fully general setting where the \\(X_i\\)'s can be arbitrarily interdependent,
notice that \\(\mu\\) must be specified by \\(O(|\mathcal{X}|^n)\\) values. However,
if the \\(X_i\\)'s are independent, there is a tremendous
simplification as this distribution factors into \\(\mathbb{P}(X_1 = x_1)
\cdots \mathbb{P}(X_n = x_n)\\),
and in this case, \\(\mu\\) is specified by only \\(O(n|\mathcal{X}|)\\)
values. By exploiting this admittedly best-case independence scenario, our
computations regarding \\(\mu\\) reduce from exponential to linear in the
dimension \\(n\\).

This is the main idea behind our graphical model representations. While it is
rare for the \\(X_i\\) to be completely independent, it is often the
case that some of the \\(X_i\\) are conditionally independent (or at least, the
assumption of such conditional independence is a reasonable approximation),
which allows a reduction in complexity and thus more efficient and feasible
algorithms.

### Example
Let's take a small detour to look an illustrative example. Lately in U.S.
domestic politics, partisanship and party alliance seem to increasingly determine an
individual's views on a given issue. This observation suggests the
approximation that issues are conditionally independent given the party
alliance is known. So suppose we pick an American[^2] at random and consider the
probability space induced by three <!-- TODO count -->
random variables *Party* \\((P)\\), *Gun Control* \\((G)\\), and *Health Care*
\\((H)\\). For the sake of simplicity we will treat these as binary random variables,
i.e.

\\[ Val(P) = \\{p^0, p^1\\} \quad Val(G) = \\{g^0, g^1\\} \quad Val(H) = \\{h^0, h^1\\} \\]

where

\begin{align}
p^0 &:= \text{republican} \\\
p^1 &:= \text{democrat} \\\
g^0 &:= \text{does not support stricter gun laws} \\\
g^1 &:= \text{supports stricter gun laws} \\\
h^0 &:= \text{does not support Medicare for all} \\\
h^1 &:= \text{supports Medicare for all}
\end{align}

As discussed above, a fully general probablistic model allows for arbitrary
\\(\mathbb{P}(P,G,H)\\) probabilities for the \\(2^n\\) possible outcomes in
this space.[^3] One such distribution is given in the table below.

| \\(P\\) | \\(G\\) | \\(H\\) | \\(\mathbb{P}(P, G, H)\\)
|:---:|:---:|:---:|:---:|
|\\(p^0\\) | \\(g^0\\) | \\(h^0\\) | \\(0.248\\)
|\\(p^0\\) | \\(g^0\\) | \\(h^1\\) | \\(0.097\\)
|\\(p^0\\) | \\(g^1\\) | \\(h^0\\) | \\(0.112\\)
|\\(p^0\\) | \\(g^1\\) | \\(h^1\\) | \\(0.043\\)
|\\(p^1\\) | \\(g^0\\) | \\(h^0\\) | \\(0.016\\)
|\\(p^1\\) | \\(g^0\\) | \\(h^1\\) | \\(0.054\\)
|\\(p^1\\) | \\(g^1\\) | \\(h^0\\) | \\(0.099\\)
|\\(p^1\\) | \\(g^1\\) | \\(h^1\\) | \\(0.331\\)
{:.marginal-table}

<!-- generated by marginals
P(g| p0) = (.31, .69)
P(h| p0) = (.28, .72)
P(g| p1) = (.86, .14)
P(h| p1) = (.77, .23)
-->

While the specific joint probabilities are contrived, the marginal
probabilities per party are based on real polling[^4], so these
values should fit roughly with our intuition.

Let's explore the dependence among these variables, first computing the
probability that an individual favors Medicare for all:
\begin{align}
 \mathbb{P}(H = h^1) &= \sum_{p, g} \mathbb{P}(H = h^1, P = p, G = g) \\\
                     &= 0.097 + 0.043 + 0.054 + 0.331 \\\
                     &= 0.525
\end{align}
So in the marginal distribution, it is more likely that an individual is in
favor of Medicare for all. However, suppose we observe that this individual
does not support stricter gun laws. How does this change the probability of the
former issue?
\begin{align}
 \mathbb{P}(H = h^1 | G = g^0)
   &= \frac{\mathbb{P}(H = h^1, G = g^0)}{\mathbb{P}(G = g^0)} \\\
   &= \frac{\sum_{p}\mathbb{P}(G = g^0, H = h^1, P=p)}
           {\sum_{p,h}\mathbb{P}(G = g^0, P=p, H=h)} \\\
   &= \frac{0.097 + 0.054}{0.248+0.097+0.016+0.054} \\\
   &= 0.364
\end{align}
The conclusion to draw from this calculation is that \\(H\\) and \\(G\\) are
quite dependent. Recall that independence holds if and only if observing
\\(G\\) does not change our beliefs about \\(H\\); in this case we see that
observing \\(G = g^0\\) rather drastically updates our beliefs, and we now find
it very unlikely that \\(H = h^1\\). So we're not at all justified in an assumption of independence.

But how exactly does the observation \\(G\\) influence \\(H\\)? Recall our
discussion above about that pesky p-word: partisanship. Let's run through the
same calculations, but this time, suppose we know the individual is a Democrat
beforehand.

\begin{align}
\mathbb{P}(H = h^1 | P = p^1)
  &= \frac{\mathbb{P}(H = h^1, P = p^1)}
          {\mathbb{P}(P = p^1)} \\\
  &= \frac{0.054 + 0.331}
          {0.500} \\\
  &= 0.77
\end{align}

Again, suppose we also observe \\(g^0\\):
\begin{align}
 \mathbb{P}(H = h^1 | G = g^0, P = p^1)
   &= \frac{\mathbb{P}(H = h^1, G = g^0, P = p^1)}
           {\mathbb{P}(G = g^0, P=p^1)} \\\
   &= \frac{0.054}{0.016 + 0.054} \\\
   &= 0.77
\end{align}

So if we already know the individual's party affiliation \\(P\\), then \\(G\\)
and
\\(H\\) are independent[^5], which we denote by \\( G \perp H \mid P \\). Thus
when observing \\(G\\) changed our beliefs about \\(H\\) in the first case, it
did so *solely* by changing our beliefs about \\(P\\). Put another way, for a
given Republican or Democrat, gun laws and nationalied health insurance are
fairly incomparable issues, but because

1. it is so unlikely that a Democrat would be against stricter gun laws
2. it is so unlikely

<!--
In the distribution laid out above, gun laws and
nationalized health insurance are fairly incomparable issues


TODO maybe avoid this and just start with a graph, explain if it factors then good
Notice that by successively applying the definition of the conditional probability mass function[^2]
[^2]: \\(\mathbb{P}(A, B) = \mathbb{P}(A)\mathbb{P}(B\mid A)\\)
we can derive another general expression

$$ \mu(x_1, \ldots, x_n) = \mu(x_1) \prod_{i = 2}^n \mu(x_i \mid x_1, \ldots, x_{i-1}). $$
h^1 -> .525
 -->

# Footnotes

[^1]: Typically a finite set, e.g. \\(\\{0,1\\}\\).
[^2]: Actually first remove the Independents and other parties, then pick a Democrat or Republican party member at random. (Sorry, it's just easier to deal with a binary random variable!)
[^3]: Technically, because these probabilities must sum to one, the distribution is fully specified by the first \\(2^n - 1\\) values.
[^4]: Opinions on gun control and health care, conditional by party, are from polling by [Pew Research Center](https://www.pewresearch.org/fact-tank/2019/10/16/share-of-americans-who-favor-stricter-gun-laws-has-increased-since-2017/ft_19-10-16_gunlaws_sizable-gender-education-differences-support-stricter-gun-laws_2/) and [Kaiser Family Foundation](https://www.kff.org/health-reform/poll-finding/kff-health-tracking-poll-november-2019/), respectively.
[^5]: I invite you to check the other cases or recall basic theorems of conditional independence and set complements.
